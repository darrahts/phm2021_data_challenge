{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6beb5e4-ff44-40d0-9be8-755385175fbf",
   "metadata": {},
   "source": [
    "# RUL Prediction with LSTM\n",
    "b. Xs' and W by flight class\n",
    "c. Xs' and W and $\\theta$\n",
    "\n",
    "1. Load flight effects model and validate performance\n",
    "2. set up hyperparameter tuning for RUL prediction network (LSTM)\n",
    "3. Evaluate different lookback windows or implement adaptive window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b781a60b-c9b1-4f94-9210-3d86063d70b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "print(base_dir)\n",
    "sys.path.insert(1, base_dir)\n",
    "from package.api import DB as api\n",
    "import package.utils as utils\n",
    "import package.tuning as tuning\n",
    "utils.check_gpu()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, optimizers, metrics\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "from kerastuner_tensorboard_logger import (\n",
    "    TensorBoardLogger,\n",
    "    setup_tb  # Optional\n",
    ")\n",
    "\n",
    "paths_df = pd.read_csv(base_dir + '/paths.csv')\n",
    "paths_df['path'] = base_dir + '/' + paths_df['path']\n",
    "\n",
    "Fc = 3\n",
    "dataset = 'DS08'\n",
    "log_location = base_dir + '/logs'\n",
    "model_location = base_dir + '/models'\n",
    "data_location = base_dir + '/data'\n",
    "data_header = f'Fc-{Fc}_dataset-{dataset}'\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# THESE ARE YOUR CREDENTIALS IN PLAIN TEXT!\n",
    "params = {'datasource.username': 'macslab', # the username of the logged in user\n",
    "            'datasource.password': 'Ch0colate!', \n",
    "            'datasource.database': 'ncmapss_db', # <- NO CHANGE \n",
    "            'datasource.url': '10.2.219.98', # <- or your database installation location\n",
    "            'datasource.port': '5432'} # <- most likely don't change\n",
    "#print(params)\n",
    "db, cur =  api.connect(params)\n",
    "db.set_session(autocommit=True)\n",
    "del(params)\n",
    "\n",
    "units_df = api._get_units(db=db)\n",
    "\n",
    "units = units_df[(units_df['Fc'] == Fc) & (units_df['dataset'].str.contains(dataset))]\n",
    "\n",
    "tables = ['summary_tb', 'telemetry_tb']\n",
    "downsample=10\n",
    "df = api._get_data(db=db,\n",
    "                   units=pd.unique(units.id),\n",
    "                   tables=tables,\n",
    "                   downsample=downsample).astype(np.float32)\n",
    "utils.add_time_column(units=pd.unique(units.id), df=df)\n",
    "utils.add_rul_column(units=pd.unique(units.id), df=df)\n",
    "\n",
    "W_cols = ['Mach', 'alt', 'TRA', 'T2', 'time']\n",
    "Xs_cols = ['Wf', 'Nf', 'Nc', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50']\n",
    "aux_cols = ['cycle', 'hs', 'Fc', 'asset_id']\n",
    "\n",
    "model = keras.models.load_model(paths_df[paths_df['name']=='flight_effects'].path.values[0])\n",
    "yscaler = joblib.load(paths_df[paths_df['name']=='flight_effects_yscaler'].path.values[0])\n",
    "xscaler = joblib.load(paths_df[paths_df['name']=='flight_effects_xscaler'].path.values[0])\n",
    "\n",
    "trace = yscaler.transform(df[Xs_cols])\n",
    "pred = model.predict(xscaler.transform(df[W_cols]))\n",
    "res = trace - pred\n",
    "dfx = pd.DataFrame(data=res, columns=Xs_cols)\n",
    "df_x = pd.DataFrame(data=xscaler.transform(df[W_cols]), columns=W_cols)\n",
    "dfx = pd.concat([dfx, df_x, df[aux_cols]], axis=1)\n",
    "dfx['rul'] = df['rul'].values\n",
    "dfx.time = dfx.time + (dfx.cycle -1)\n",
    "dfx0 = dfx[dfx.hs == 0]\n",
    "\n",
    "train_df, train_y, val_df, val_y, test_df, test_y = utils.train_test_split(df=dfx0, units=list(pd.unique(units.id)), y_labels=['rul'], t_labels=aux_cols + Xs_cols + W_cols, train_pct = 1.0, val_pct=0.0, test_pct=0.0, verbose=True)\n",
    "train_y = pd.DataFrame(data=train_y, columns=['rul'])\n",
    "train_pop = pd.concat([train_df.pop(x) for x in ['asset_id', 'cycle', 'hs', 'Fc']], axis=1)\n",
    "\n",
    "lookback = 500\n",
    "horizon = 1\n",
    "n_out = 1\n",
    "n_features = len(important_features)\n",
    "\n",
    "X_train, y_train = utils.temporalize_data(train_df[important_features].values, train_y.values, lookback, horizon, n_features, n_out)\n",
    "train_pop_temp, y_pop = utils.temporalize_data(train_pop.values, train_y, lookback, horizon, len(train_pop.columns), n_out)\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689467d3-fcff-439b-a8cd-a44399f1176e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a7ec2-2d5d-4768-9d02-f45ae4f3056e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab79e7f-0c09-4674-8969-ebbfa1b8ff4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc9a5983-d9fd-488c-9f20-b5a5a774bdb2",
   "metadata": {},
   "source": [
    "## 1. Make Xs_prime and y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d071778-ef11-4406-9496-d4a886b4faae",
   "metadata": {},
   "source": [
    "## 2. split sets and view feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af1aa0be-a4df-4861-827d-537ccb11aa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "0\n",
      "0\n",
      "train, val, test set counts: 10, 0, 0\n",
      "10 10\n",
      "train units: [83, 85, 81, 71, 78, 67, 77, 82, 84, 79]\n",
      "val units: []\n",
      "test units: []\n"
     ]
    }
   ],
   "source": [
    "train_df, train_y, val_df, val_y, test_df, test_y = utils.train_test_split(df=dfx0, units=list(pd.unique(units.id)), y_labels=['rul'], t_labels=aux_cols + Xs_cols + W_cols, train_pct = 1.0, val_pct=0.0, test_pct=0.0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12823f93-9bbb-4661-81c6-563cbac763c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.DataFrame(data=train_y, columns=['rul'])\n",
    "val_y = pd.DataFrame(data=val_y, columns=['rul'])\n",
    "test_y = pd.DataFrame(data=test_y, columns=['rul'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1681d9-87fb-42cd-acb1-2acead2553db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pop = pd.concat([train_df.pop(x) for x in ['asset_id', 'cycle', 'hs', 'Fc']], axis=1)\n",
    "val_pop = pd.concat([val_df.pop(x) for x in ['asset_id', 'cycle', 'hs', 'Fc']], axis=1)\n",
    "test_pop = pd.concat([test_df.pop(x) for x in ['asset_id', 'cycle', 'hs', 'Fc']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71440d4b-5b5f-4b22-a0b6-8c04bef0187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df.to_csv(f'{data_location}/train_df_{data_header}.csv')\n",
    "train_pop.to_csv(f'{data_location}/train_pop_{data_header}.csv')\n",
    "train_y.to_csv(f'{data_location}/train_y_{data_header}.csv')\n",
    "\n",
    "val_df.to_csv(f'{data_location}/val_df_{data_header}.csv')\n",
    "val_pop.to_csv(f'{data_location}/val_pop_{data_header}.csv')\n",
    "val_y.to_csv(f'{data_location}/val_y_{data_header}.csv')\n",
    "\n",
    "test_df.to_csv(f'{data_location}/test_df_{data_header}.csv')\n",
    "test_pop.to_csv(f'{data_location}/test_pop_{data_header}.csv')\n",
    "test_y.to_csv(f'{data_location}/test_y_{data_header}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a676b-fad4-4b5f-8978-1f99f7e9cfca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d7d27-9853-4cf9-9808-acf9a5ff574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.plot_feature_distributions(df=train_df, feature_range=(0,1), figsize=(12,4))\n",
    "#utils.plot_feature_distributions(df=val_df, feature_range=(0,1), figsize=(12,4))\n",
    "#utils.plot_feature_distributions(df=test_df, feature_range=(0,1), figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606b296-b6d4-47eb-a4e0-3edd4cf70d50",
   "metadata": {},
   "source": [
    "### ~plot feature importances~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ec7bf-78fe-42b4-92cd-06cb23ca0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "# importance_model = XGBRegressor()\n",
    "# importance_model.fit(train_df.values, train_y)\n",
    "\n",
    "# scoring = ['explained_variance',\n",
    "#            'max_error']#,\n",
    "#            #'neg_mean_absolute_error',\n",
    "#            #'neg_root_mean_squared_error',\n",
    "#            #'r2']\n",
    "\n",
    "# utils.plot_feature_importances(model=importance_model,\n",
    "#                                features=train_df.values,\n",
    "#                                feature_labels=Xs_cols + W_cols,\n",
    "#                                target=train_y,\n",
    "#                                target_label='rul',\n",
    "#                                scoring=scoring)\n",
    "\n",
    "# utils.plot_feature_importance(features=train_df.values,\n",
    "#                               feature_labels=Xs_cols + W_cols,\n",
    "#                               target=train_y,\n",
    "#                               target_label='rul',\n",
    "#                               figsize=(9,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8654274-0608-477f-938d-732412426c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #important_features = list(train_df.columns) #['Wf','Nc', 'T24', 'T48', 'T50', 'P2', 'Ps30', 'P40']\n",
    "# important_features = ['Wf','Nf', 'T30', 'T48', 'T50', 'P15', 'P24', 'P40', 'P50','TRA', 'T2', 'time']\n",
    "# important_features\n",
    "len(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b8d6e-21d1-43c4-81e4-e6cc179d3433",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5674cb-32ff-413f-85eb-801814fd8ecc",
   "metadata": {},
   "source": [
    "## 2. temporalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9dfe6-5e65-4c75-bc99-b70de90bacde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#important_features = list(train_df.columns)\n",
    "\n",
    "lookback = 500\n",
    "horizon = 1\n",
    "n_out = 1\n",
    "n_features = len(important_features)\n",
    "\n",
    "X_train, y_train = utils.temporalize_data(train_df[important_features].values, train_y.values, lookback, horizon, n_features, n_out)\n",
    "train_pop_temp, y_pop = utils.temporalize_data(train_pop.values, train_y, lookback, horizon, len(train_pop.columns), n_out)\n",
    "\n",
    "X_val, y_val = utils.temporalize_data(val_df.values, val_y, lookback, horizon, n_features, n_out)\n",
    "val_pop_temp, y_pop = utils.temporalize_data(val_pop.values, val_y, lookback, horizon, n_features, n_out)\n",
    "\n",
    "X_test, y_test = utils.temporalize_data(test_df.values, test_y, lookback, horizon, n_features, n_out)\n",
    "test_pop_temp, y_pop = utils.temporalize_data(test_pop.values, test_y, lookback, horizon, n_features, n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3f9b4-c95c-41fd-8aa4-651760a0dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{data_location}/{data_header}_X.npy', X_train)\n",
    "np.save(f'{data_location}/{data_header}_y.npy', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9c01a-f86e-4435-a89c-0a1411429021",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ca495-04ab-4567-8317-821fee22f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(f'{data_location}/{data_header}_X_train.npy').astype('float32')\n",
    "y_train = np.load(f'{data_location}/{data_header}_y_train.npy').astype('float32')\n",
    "\n",
    "X_test = np.load(f'{data_location}/{data_header}_X_test.npy').astype('float32')\n",
    "y_test = np.load(f'{data_location}/{data_header}_y_test.npy').astype('float32')\n",
    "\n",
    "X_val = np.load(f'{data_location}/{data_header}_X_val.npy').astype('float32')\n",
    "y_val = np.load(f'{data_location}/{data_header}_y_val.npy').astype('float32')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6bc46-a398-4929-8703-261978ae169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([X_train, X_test, X_val])\n",
    "y = np.vstack([y_train, y_test, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439df233-7f0b-49be-9be7-192f6d30946b",
   "metadata": {},
   "source": [
    "## Parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1bae2-cb33-4d07-909d-a8bb398df811",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf38596-72b6-43c2-96e5-0ee3dd460b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lookback = 100\n",
    "horizon = 1\n",
    "n_out = 1\n",
    "n_features = X.shape[2]\n",
    "\n",
    "footer = 'retry'\n",
    "input_shape = (lookback, n_features)\n",
    "my_tuning = tuning.Tuning(input_shape, n_out)\n",
    "bayesian_tuning = my_tuning.bayesian_search(objective='val_root_mean_squared_error',\n",
    "                                            mode='min',\n",
    "                                            max_trials=96,\n",
    "                                            alpha=.0005,\n",
    "                                            beta=10,\n",
    "                                            epochs=5,\n",
    "                                            executions_per_trial=1,\n",
    "                                            hypermodel=my_tuning.create_bilstm_hypermodel,\n",
    "                                            directory=f'{log_location}/{data_header}_{footer}',\n",
    "                                            project_name='bilstm',\n",
    "                                            logger=TensorBoardLogger(\n",
    "                                                metrics=['val_root_mean_squared_error'],\n",
    "                                                         logdir=f'{log_location}/{data_header}_{footer}/hparams'\n",
    "                                            ),\n",
    "                                            X=X_train,\n",
    "                                            y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547879d8-1406-4397-8f01-a6492c255cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_tuning_params = bayesian_tuning.get_best_hyperparameters(num_trials=1)[0]\n",
    "bayesian_tuning_model = bayesian_tuning.get_best_models()[0]\n",
    "print(bayesian_tuning_params.values)\n",
    "bayesian_tuning_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9afede-63e1-4bac-b727-9df43ef471c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_tuning_model.save(f'{model_location}/{data_header}_best.h5')\n",
    "bayesian_tuning_model.save(f'{model_location}/{data_header}_best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc3bb9-9faf-4f4d-970a-444586483845",
   "metadata": {},
   "source": [
    "# scratch / misc below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b720c9d-4184-42e7-bd01-b62f82590337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xs_prime = []\n",
    "# idx = np.random.randint(len(Xs_cols))\n",
    "# polys = []\n",
    "\n",
    "# df[W_cols] = xscaler.transform(df[W_cols])\n",
    "# pred = model.predict(df[W_cols])\n",
    "\n",
    "# for asset_id in pd.unique(df.asset_id):\n",
    "#     X_full = df[df.asset_id == asset_id][W_cols]\n",
    "#     pred = model.predict(X_full)\n",
    "#     # pred = model.predict(xscaler.transform(X_full))\n",
    "#     Xs_prime.append(pred)\n",
    "    \n",
    "#    y_full = df[df.asset_id == asset_id][Xs_cols]\n",
    "#    trace = yscaler.transform(y_full)\n",
    "#     res = trace - pred\n",
    "\n",
    "#     x = np.arange(0, len(res[:,0]))\n",
    "#     x = x[0:len(x):10]\n",
    "#     for i in range(0, len(Xs_cols)):\n",
    "#         y = res[:,i]\n",
    "#         y = y[0:len(y):10]\n",
    "#         reg = np.poly1d(np.polyfit(x, y, 6))\n",
    "#         polys.append(reg)\n",
    "\n",
    "#     fig = plt.figure(figsize=(12,12))\n",
    "#     filtered = np.zeros(res.shape)\n",
    "#     x = np.arange(0, len(res[:,0]))\n",
    "#     for j in range(len(Xs_cols)):\n",
    "#         y = res[:,j]\n",
    "#         p = polys[j]\n",
    "\n",
    "#         diff = np.zeros((len(y),))\n",
    "#         for i in range(len(y)):\n",
    "#             predp = p(y[i])\n",
    "#             diff[i] = y[i] - predp\n",
    "\n",
    "#         mean = np.mean(diff)\n",
    "#         std = np.std(diff)\n",
    "\n",
    "#         filt = np.zeros((len(y),))\n",
    "#         for i in range(len(res)):\n",
    "#             if np.abs(np.abs(y[i]) - np.abs(p(x[i]))) > np.abs(mean)+3*std:\n",
    "#                 filt[i] = p(x[i])\n",
    "#             else:\n",
    "#                 filt[i] = y[i]\n",
    "\n",
    "#         filtered[:,j] = filt\n",
    "#         utils.plot_trace_samples(fig, y, filt, p, j)\n",
    "#     plt.show()\n",
    "#     Xs_prime.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a76dd-17a9-4a09-916f-6e0a9e78a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Queue\n",
    "# from threading import Thread\n",
    "\n",
    "# units = api._get_units(db=db)\n",
    "# tables = ['summary_tb', 'telemetry_tb']\n",
    "\n",
    "# def func(units, tables, downsample):\n",
    "#     return str(units) + str(tables) + str(downsample)\n",
    "# def delegate():\n",
    "#     return func('units', ' tables', 20)\n",
    "# units = api._get_units(db=db)\n",
    "# data_frames = []\n",
    "# for i in range(int(len(units)/10)):\n",
    "    \n",
    "    \n",
    "    # df = api._get_data(db=db,\n",
    "    #                    units=units,\n",
    "    #                    tables=tables,\n",
    "    #                    downsample=20).astype(np.float32)\n",
    "    \n",
    "    \n",
    "# train_df = pd.read_csv('train_df.csv', index_col=0)\n",
    "# train_pop = pd.read_csv('train_pop.csv', index_col=0)\n",
    "# train_y = pd.read_csv('train_y.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161c58a-e2d2-4279-86b4-4abe627483a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "input_shape = (lookback, n_features)\n",
    "my_tuning = tuning.Tuning(input_shape, n_out)\n",
    "\n",
    "params = tuning.MyParameters(layers=3, units=64, dropout_rate=0, learning_rate=.001, recurrent_dropout=0,l2=-1)\n",
    "\n",
    "\n",
    "model = my_tuning.build_lstm_model(params)\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
