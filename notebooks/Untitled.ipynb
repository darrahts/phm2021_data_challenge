{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27d45c0d-1039-4584-86e1-3c3f4800292d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\Dropbox\\NASA\\phm2021_data_challenge\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import numpy as np\n",
    "import os, sys\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "print(base_dir)\n",
    "sys.path.insert(1, base_dir)\n",
    "import package.utils as utils\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss = loss_fcn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_error_metric.update_state(y, y_pred)\n",
    "    return loss\n",
    "    \n",
    "    \n",
    "@tf.function\n",
    "def val_step(x,y):\n",
    "    y_pred = model(x, training=False)\n",
    "    val_error_metric.update_state(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d93274-a92e-404d-be9c-7ebef0dbef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 7\n",
    "num_layers = 3\n",
    "num_outputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9a66c65-4d47-462c-8f18-586575a3a5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\Dropbox\\NASA\\phm2021_data_challenge\n",
      "2.3.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e4e419fcbd98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#import package.tuning as tuning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_gpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Dropbox\\NASA\\phm2021_data_challenge\\package\\utils.py\u001b[0m in \u001b[0;36mcheck_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mgpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mhas_gpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'GPU'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"[INFO] GPU?: <{has_gpu}> {gpu}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhas_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "print(base_dir)\n",
    "sys.path.insert(1, base_dir)\n",
    "from package.api import DB as api\n",
    "import package.utils as utils\n",
    "#import package.tuning as tuning\n",
    "utils.check_gpu()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, optimizers, metrics\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "# import keras_tuner as kt\n",
    "\n",
    "# from kerastuner_tensorboard_logger import (\n",
    "#     TensorBoardLogger,\n",
    "#     setup_tb  # Optional\n",
    "# )\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "paths_df = pd.read_csv(base_dir + '/paths.csv')\n",
    "paths_df['path'] = base_dir + '/' + paths_df['path']\n",
    "\n",
    "Fc = 3\n",
    "dataset = 'DS08'\n",
    "\n",
    "log_location = base_dir + '/logs'\n",
    "model_location = base_dir + '/models'\n",
    "data_location = base_dir + '/data'\n",
    "data_header = f'Fc-{Fc}_dataset-{dataset}'\n",
    "\n",
    "\n",
    "\n",
    "params = {'datasource.username': 'macslab', # the username of the logged in user\n",
    "            'datasource.password': 'Ch0colate!', \n",
    "            'datasource.database': 'ncmapss_db', # <- NO CHANGE \n",
    "            'datasource.url': '10.2.219.98', # <- or your database installation location\n",
    "            'datasource.port': '5432'} # <- most likely don't change\n",
    "db, cur =  api.connect(params)\n",
    "db.set_session(autocommit=True)\n",
    "del(params)\n",
    "\n",
    "\n",
    "units_df = api._get_units(db=db)\n",
    "units = units_df[(units_df['Fc'] == Fc) & (units_df['dataset'].str.contains(dataset))]\n",
    "\n",
    "\n",
    "tables = ['summary_tb', 'telemetry_tb']\n",
    "downsample=10\n",
    "df = api._get_data(db=db,\n",
    "                   units=pd.unique(units.id),\n",
    "                   tables=tables,\n",
    "                   downsample=downsample).astype(np.float32)\n",
    "utils.add_time_column(units=pd.unique(units.id), df=df)\n",
    "utils.add_rul_column(units=pd.unique(units.id), df=df)\n",
    "\n",
    "\n",
    "W_cols = ['Mach', 'alt', 'TRA', 'T2', 'time']\n",
    "Xs_cols = ['Wf', 'Nf', 'Nc', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50']\n",
    "aux_cols = ['cycle', 'hs', 'Fc', 'asset_id']\n",
    "\n",
    "model = keras.models.load_model(paths_df[paths_df['name']=='flight_effects'].path.values[0])\n",
    "yscaler = joblib.load(paths_df[paths_df['name']=='flight_effects_yscaler'].path.values[0])\n",
    "xscaler = joblib.load(paths_df[paths_df['name']=='flight_effects_xscaler'].path.values[0])\n",
    "\n",
    "\n",
    "trace = yscaler.transform(df[Xs_cols])\n",
    "pred = model.predict(xscaler.transform(df[W_cols]))\n",
    "res = trace - pred\n",
    "dfx = pd.DataFrame(data=res, columns=Xs_cols)\n",
    "df_x = pd.DataFrame(data=xscaler.transform(df[W_cols]), columns=W_cols)\n",
    "dfx = pd.concat([dfx, df_x, df[aux_cols]], axis=1)\n",
    "dfx['rul'] = df['rul'].values\n",
    "dfx.time = dfx.time + (dfx.cycle -1)\n",
    "dfx0 = dfx[dfx.hs == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "922622cd-e28a-498e-83f2-cd3d65a0a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (None, num_features)\n",
    "\n",
    "inputs = keras.Input(shape=input_shape, name='in1')\n",
    "\n",
    "#x = layers.Masking(mask_value=0., input_shape=input_shape)(inputs)\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(units=32,\n",
    "                                    kernel_regularizer=regularizers.l1_l2(l1=.00001, l2=.00001),\n",
    "                                    return_sequences=True,\n",
    "                                    name='hidden_0'))(inputs)\n",
    "\n",
    "x = layers.Dropout(rate=.2)(x)\n",
    "\n",
    "for i in range(1, num_layers-1):\n",
    "    x = layers.Bidirectional(layers.LSTM(units=32,\n",
    "                                        kernel_regularizer=regularizers.l1_l2(l1=.00001, l2=.00001),\n",
    "                                        return_sequences=True,\n",
    "                                        name=f'hidden_{i}'))(x)\n",
    "    \n",
    "    x = layers.Dropout(rate=.2)(x)\n",
    "    \n",
    "x = layers.Bidirectional(layers.LSTM(units=32,\n",
    "                                    kernel_regularizer=regularizers.l1_l2(l1=.00001, l2=.00001),\n",
    "                                        return_sequences=False,\n",
    "                                        name=f'hidden_{i}'))(x)\n",
    "    \n",
    "x = layers.Dropout(rate=.2)(x)\n",
    "\n",
    "outputs = layers.Dense(num_outputs)(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c7a7a-6d92-45d3-962d-bdb2a75cc6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### define the random test data\n",
    "\n",
    "num_samples = 5000\n",
    "X = np.random.random((num_samples, num_features))\n",
    "y = np.random.random((num_samples,))\n",
    "\n",
    "\n",
    "####     define sequence range\n",
    "\n",
    "min_seq_len = 100\n",
    "max_seq_len = 300\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "\n",
    "\n",
    "####################################################\n",
    "####            training objects\n",
    "####################################################\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=.001)\n",
    "loss_fcn = keras.losses.MeanSquaredError()\n",
    "train_error_metric = keras.metrics.RootMeanSquaredError()\n",
    "val_error_metric = keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "\n",
    "####################################################\n",
    "####            main training loop\n",
    "####################################################\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ## get a random sequence length and number of batches\n",
    "    seq_len = np.random.randint(min_seq_len, max_seq_len)\n",
    "    num_batches = round((X.shape[0] - seq_len) / batch_size) + 1\n",
    "    print(f\"seq_len = {seq_len}, num_batches = {num_batches}\")\n",
    "    \n",
    "    ## index counters for iterating the data and creating batches on the fly\n",
    "    start = 0\n",
    "    stop = seq_len + batch_size + start\n",
    "    \n",
    "    ## inner loop to iterate within an epoch\n",
    "    for batch in tqdm(range(num_batches)):\n",
    "        \n",
    "        ## initialize the batches\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        ## used to prevent the counters going beyond the length of the data\n",
    "        break_signal = False\n",
    "        \n",
    "        ## inner loop to iterate within a batch\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            ## initialize the samples\n",
    "            X_sample = []\n",
    "            y_sample = []\n",
    "            \n",
    "            ## builds X_sample\n",
    "            for j in range(seq_len):\n",
    "                _x = X[start:stop]\n",
    "                if i+j < len(_x):\n",
    "                    X_sample.append(_x[i+j])\n",
    "\n",
    "            ## builds y_sample, prevents index out of range\n",
    "            for j in range(seq_len, seq_len+horizon):\n",
    "                idx = (batch*batch_size)+i+j\n",
    "                if idx < len(y):\n",
    "                    y_sample.append(y[idx])\n",
    "                else:\n",
    "                    break_signal = True\n",
    "        \n",
    "            if break_signal:\n",
    "                break\n",
    "        \n",
    "            ## adds the sample to the batch\n",
    "            X_batch.append(X_sample)\n",
    "            y_batch.append(y_sample)\n",
    "            \n",
    "        ## increments the counters for iterating over the data\n",
    "        start = stop - seq_len\n",
    "        stop = start + seq_len + batch_size\n",
    "        \n",
    "        ## converts the batches to np arrays\n",
    "        X_batch = np.array(X_batch)\n",
    "        y_batch = np.array(y_batch)\n",
    "        \n",
    "        ## there are some cases where the final batch is empty\n",
    "        if len(y_batch) > 0:\n",
    "            \n",
    "            ## train step, uses the last batch of each epoch for validation\n",
    "            if batch < num_batches - 1:\n",
    "                loss = train_step(X_batch, y_batch)\n",
    "                \n",
    "    ## update the train error at the end of each epoch\n",
    "    train_error = train_error_metric.result()\n",
    "    print(f\"training error on epoch {epoch}: {float(train_error)}\")\n",
    "    train_error_metric.reset_states()\n",
    "    \n",
    "    ## update the validation error at the end of each epoch\n",
    "    val_step(X_batch, y_batch)\n",
    "    \n",
    "    val_error = val_error_metric.result()\n",
    "    print(f\"validation error on epoch {epoch}: {float(val_error)}\")\n",
    "    val_error_metric.reset_states()\n",
    "    print(f\"elapsed time: {time.time() - start_time}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8176e-8b6c-4109-8378-f4481b5d749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb27533-2c0d-4591-aa1d-573ec063c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    if i == 10 - 1:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00560cd9-195d-4610-8c63-ea8afbd573b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random((101, 3))\n",
    "y = np.random.random((101,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d8fa7-ca71-4aa4-9f46-cde21faabe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "batch_size = 7\n",
    "start = 0\n",
    "num_batches = round((X.shape[0] - seq_len) / batch_size)\n",
    "stop = seq_len + batch_size + start\n",
    "\n",
    "for batch in range(num_batches):\n",
    "    \n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    for i in range(batch_size):\n",
    "        X_sample = []\n",
    "        y_sample = []\n",
    "        for j in range(seq_len):\n",
    "            X_sample.append(X[start:stop][i+j])\n",
    "            \n",
    "        for j in range(seq_len, seq_len+horizon):\n",
    "            y_sample.append(y[(batch*batch_size)+i+j])\n",
    "        \n",
    "        X_batch.append(X_sample)\n",
    "        y_batch.append(y_sample)\n",
    "        \n",
    "\n",
    "\n",
    "    start = stop - seq_len\n",
    "    stop = start + seq_len + batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd8b7e-155b-4d7f-934e-f06f1c0bca8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003b7c5-0812-4bda-870c-3ddbafd00717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eae33e-630e-43d2-932e-9eb8d8649716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be9cec-9d38-49a3-ace8-76865104de47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846da08-a1b5-4f8d-ae27-2e51cb018956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3a7cf-9da1-4ca7-b533-1a459be3cc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e2962-5907-409a-a930-cff6f886aa9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
