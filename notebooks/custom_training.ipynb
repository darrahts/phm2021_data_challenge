{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72987a3f-6644-4d8b-a2ac-a00790f9db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "print(base_dir)\n",
    "sys.path.insert(1, base_dir)\n",
    "from package.api import DB as api\n",
    "import package.utils as utils\n",
    "import package.tuning as tuning\n",
    "utils.check_gpu()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, optimizers, metrics\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "from kerastuner_tensorboard_logger import (\n",
    "    TensorBoardLogger,\n",
    "    setup_tb  # Optional\n",
    ")\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "paths_df = pd.read_csv(base_dir + '/paths.csv')\n",
    "paths_df['path'] = base_dir + '/' + paths_df['path']\n",
    "\n",
    "Fc = 1\n",
    "dataset = 'DS08'\n",
    "\n",
    "log_location = base_dir + '/logs'\n",
    "model_location = base_dir + '/models'\n",
    "data_location = base_dir + '/data'\n",
    "data_header = f'Fc-{Fc}_dataset-{dataset}'\n",
    "\n",
    "\n",
    "\n",
    "params = {'datasource.username': 'macslab', # the username of the logged in user\n",
    "            'datasource.password': 'Ch0colate!', \n",
    "            'datasource.database': 'ncmapss_db', # <- NO CHANGE \n",
    "            'datasource.url': '10.2.219.98', # <- or your database installation location\n",
    "            'datasource.port': '5432'} # <- most likely don't change\n",
    "db, cur =  api.connect(params)\n",
    "db.set_session(autocommit=True)\n",
    "del(params)\n",
    "\n",
    "\n",
    "units_df = api._get_units(db=db)\n",
    "units = units_df[(units_df['Fc'] == Fc) & (units_df['dataset'].str.contains(dataset))]\n",
    "\n",
    "\n",
    "tables = ['summary_tb', 'telemetry_tb']\n",
    "downsample=10\n",
    "df = api._get_data(db=db,\n",
    "                   units=pd.unique(units.id),\n",
    "                   tables=tables,\n",
    "                   downsample=downsample).astype(np.float32)\n",
    "utils.add_time_column(units=pd.unique(units.id), df=df)\n",
    "utils.add_rul_column(units=pd.unique(units.id), df=df)\n",
    "\n",
    "\n",
    "W_cols = ['Mach', 'alt', 'TRA', 'T2', 'time']\n",
    "Xs_cols = ['Wf', 'Nf', 'Nc', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50']\n",
    "aux_cols = ['cycle', 'hs', 'Fc', 'asset_id']\n",
    "\n",
    "model = keras.models.load_model(paths_df[paths_df['name']=='flight_effects'].path.values[0])\n",
    "yscaler = joblib.load(paths_df[paths_df['name']=='flight_effects_yscaler'].path.values[0])\n",
    "xscaler = joblib.load(paths_df[paths_df['name']=='flight_effects_xscaler'].path.values[0])\n",
    "\n",
    "\n",
    "trace = yscaler.transform(df[Xs_cols])\n",
    "pred = model.predict(xscaler.transform(df[W_cols]))\n",
    "res = trace - pred\n",
    "dfx = pd.DataFrame(data=res, columns=Xs_cols)\n",
    "df_x = pd.DataFrame(data=xscaler.transform(df[W_cols]), columns=W_cols)\n",
    "dfx = pd.concat([dfx, df_x, df[aux_cols]], axis=1)\n",
    "dfx['rul'] = df['rul'].values\n",
    "dfx.time = dfx.time + (dfx.cycle -1)\n",
    "dfx0 = dfx[dfx.hs == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0732cf2e-f046-4094-866b-5d23385c98cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss = loss_fcn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_error_metric.update_state(y, y_pred)\n",
    "    return loss\n",
    "    \n",
    "    \n",
    "@tf.function\n",
    "def val_step(x,y):\n",
    "    y_pred = model(x, training=False)\n",
    "    val_error_metric.update_state(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb843fca-850d-4824-9d13-3f17cb42f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####     define sequence range\n",
    "\n",
    "min_seq_len = 300\n",
    "max_seq_len = 1000\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "####################################################\n",
    "####            training objects\n",
    "####################################################\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=.001)\n",
    "loss_fcn = keras.losses.MeanSquaredError()\n",
    "train_error_metric = keras.metrics.RootMeanSquaredError()\n",
    "val_error_metric = keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "\n",
    "####################################################\n",
    "####            main training loop\n",
    "####################################################\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ## get a random sequence length and number of batches\n",
    "    seq_len = np.random.randint(min_seq_len, max_seq_len)\n",
    "    num_batches = round((X.shape[0] - seq_len) / batch_size) + 1\n",
    "    print(f\"seq_len = {seq_len}, num_batches = {num_batches}\")\n",
    "    \n",
    "    ## index counters for iterating the data and creating batches on the fly\n",
    "    start = 0\n",
    "    stop = seq_len + batch_size + start\n",
    "    \n",
    "    ## inner loop to iterate within an epoch\n",
    "    for batch in tqdm(range(num_batches)):\n",
    "        \n",
    "        ## initialize the batches\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        ## used to prevent the counters going beyond the length of the data\n",
    "        break_signal = False\n",
    "        \n",
    "        ## inner loop to iterate within a batch\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            ## initialize the samples\n",
    "            X_sample = []\n",
    "            y_sample = []\n",
    "            \n",
    "            ## builds X_sample\n",
    "            for j in range(seq_len):\n",
    "                _x = X[start:stop]\n",
    "                if i+j < len(_x):\n",
    "                    X_sample.append(_x[i+j])\n",
    "\n",
    "            ## builds y_sample, prevents index out of range\n",
    "            for j in range(seq_len, seq_len+horizon):\n",
    "                idx = (batch*batch_size)+i+j\n",
    "                if idx < len(y):\n",
    "                    y_sample.append(y[idx])\n",
    "                else:\n",
    "                    break_signal = True\n",
    "        \n",
    "            if break_signal:\n",
    "                break\n",
    "        \n",
    "            ## adds the sample to the batch\n",
    "            X_batch.append(X_sample)\n",
    "            y_batch.append(y_sample)\n",
    "            \n",
    "        ## increments the counters for iterating over the data\n",
    "        start = stop - seq_len\n",
    "        stop = start + seq_len + batch_size\n",
    "        \n",
    "        ## converts the batches to np arrays\n",
    "        X_batch = np.array(X_batch)\n",
    "        y_batch = np.array(y_batch)\n",
    "        \n",
    "        ## there are some cases where the final batch is empty\n",
    "        if len(y_batch) > 0:\n",
    "            \n",
    "            ## train step, uses the last batch of each epoch for validation\n",
    "            if batch < num_batches - 1:\n",
    "                loss = train_step(X_batch, y_batch)\n",
    "                \n",
    "    ## update the train error at the end of each epoch\n",
    "    train_error = train_error_metric.result()\n",
    "    print(f\"training error on epoch {epoch}: {float(train_error)}\")\n",
    "    train_error_metric.reset_states()\n",
    "    \n",
    "    ## update the validation error at the end of each epoch\n",
    "    val_step(X_batch, y_batch)\n",
    "    \n",
    "    val_error = val_error_metric.result()\n",
    "    print(f\"validation error on epoch {epoch}: {float(val_error)}\")\n",
    "    val_error_metric.reset_states()\n",
    "    print(f\"elapsed time: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e438865-2956-4b28-a325-9fffb4b712e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8a1c8-e1bb-49c8-8b49-d2f169642d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
